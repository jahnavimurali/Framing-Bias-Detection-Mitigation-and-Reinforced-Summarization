{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T14:48:25.224094Z",
     "iopub.status.busy": "2025-02-21T14:48:25.223694Z",
     "iopub.status.idle": "2025-02-21T14:48:25.232834Z",
     "shell.execute_reply": "2025-02-21T14:48:25.231980Z",
     "shell.execute_reply.started": "2025-02-21T14:48:25.224063Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ddp_longformer_bilstm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_longformer_bilstm.py\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Processing with Event Context\n",
    "# -----------------------------\n",
    "class BiasDataset(Dataset):\n",
    "    \"\"\" Custom dataset for bias classification with event-level context \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "\n",
    "        sentence_encoding = self.tokenizer(row['sentence'], truncation=True, padding=\"max_length\",\n",
    "                                           max_length=self.max_length, return_tensors=\"pt\")\n",
    "        article_encoding = self.tokenizer(row['article_text'], truncation=True, padding=\"max_length\",\n",
    "                                          max_length=self.max_length, return_tensors=\"pt\")\n",
    "        ev1_encoding = self.tokenizer(row['ev_1'], truncation=True, padding=\"max_length\",\n",
    "                                      max_length=self.max_length, return_tensors=\"pt\")\n",
    "        ev2_encoding = self.tokenizer(row['ev_2'], truncation=True, padding=\"max_length\",\n",
    "                                      max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"sentence_input_ids\": sentence_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"sentence_attention_mask\": sentence_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"article_input_ids\": article_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"article_attention_mask\": article_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"ev1_input_ids\": ev1_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"ev1_attention_mask\": ev1_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"ev2_input_ids\": ev2_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"ev2_attention_mask\": ev2_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# Model: LongformerBiLSTM with Event Context\n",
    "# -----------------------------\n",
    "class LongformerBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, num_layers=2, num_classes=1, fine_tune_layers=2):\n",
    "        super(LongformerBiLSTM, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "        # Freeze All Layers First\n",
    "        for param in self.longformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze Last Few Layers\n",
    "        for param in self.longformer.encoder.layer[-fine_tune_layers:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.longformer.config.hidden_size, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 8, 256),  # Increased size for event-level embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        _, (hidden, _) = self.lstm(last_hidden_state)\n",
    "        embedding = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, article_input_ids, article_attention_mask, sentence_input_ids, sentence_attention_mask,\n",
    "                ev1_input_ids, ev1_attention_mask, ev2_input_ids, ev2_attention_mask):\n",
    "        primary_embedding = self.encode_text(article_input_ids, article_attention_mask)\n",
    "        sentence_embedding = self.encode_text(sentence_input_ids, sentence_attention_mask)\n",
    "        ev1_embedding = self.encode_text(ev1_input_ids, ev1_attention_mask)\n",
    "        ev2_embedding = self.encode_text(ev2_input_ids, ev2_attention_mask)\n",
    "\n",
    "        combined_features = torch.cat([sentence_embedding, primary_embedding, ev1_embedding, ev2_embedding], dim=1)\n",
    "        logits = self.classifier(combined_features).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# Loss Function\n",
    "# -----------------------------\n",
    "def get_weighted_loss(train_df, device):\n",
    "    num_pos = train_df['label'].sum()\n",
    "    num_neg = len(train_df) - num_pos\n",
    "    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32).to(device)\n",
    "    return BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            sentence_input_ids = batch[\"sentence_input_ids\"].to(device)\n",
    "            sentence_attention_mask = batch[\"sentence_attention_mask\"].to(device)\n",
    "            article_input_ids = batch[\"article_input_ids\"].to(device)\n",
    "            article_attention_mask = batch[\"article_attention_mask\"].to(device)\n",
    "            ev1_input_ids = batch[\"ev1_input_ids\"].to(device)\n",
    "            ev1_attention_mask = batch[\"ev1_attention_mask\"].to(device)\n",
    "            ev2_input_ids = batch[\"ev2_input_ids\"].to(device)\n",
    "            ev2_attention_mask = batch[\"ev2_attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(article_input_ids, article_attention_mask, sentence_input_ids, sentence_attention_mask,\n",
    "                           ev1_input_ids, ev1_attention_mask, ev2_input_ids, ev2_attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(logits).round().cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average=\"binary\", zero_division=0)\n",
    "    return total_loss / len(dataloader), acc, precision, recall, f1\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, grad_accum_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    loop = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\", leave=False)\n",
    "\n",
    "    for i, batch in loop:\n",
    "        sentence_input_ids = batch[\"sentence_input_ids\"].to(device)\n",
    "        sentence_attention_mask = batch[\"sentence_attention_mask\"].to(device)\n",
    "        article_input_ids = batch[\"article_input_ids\"].to(device)\n",
    "        article_attention_mask = batch[\"article_attention_mask\"].to(device)\n",
    "        ev1_input_ids = batch[\"ev1_input_ids\"].to(device)\n",
    "        ev1_attention_mask = batch[\"ev1_attention_mask\"].to(device)\n",
    "        ev2_input_ids = batch[\"ev2_input_ids\"].to(device)\n",
    "        ev2_attention_mask = batch[\"ev2_attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(article_input_ids, article_attention_mask, sentence_input_ids, sentence_attention_mask,\n",
    "                       ev1_input_ids, ev1_attention_mask, ev2_input_ids, ev2_attention_mask)\n",
    "        loss = criterion(logits, labels) / grad_accum_steps\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "        if (i + 1) % grad_accum_steps == 0 or (i + 1) == len(dataloader):\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item() * grad_accum_steps)\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main DDP Training Function\n",
    "# -----------------------------\n",
    "def train_ddp(args):\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "    # Load Data \n",
    "    articles_df = pd.read_excel(\"/kaggle/input/basil-dataset/articles.xlsx\")\n",
    "    bias_df = pd.read_excel(\"/kaggle/input/basil-dataset/labeled_dataset.xlsx\")\n",
    "\n",
    "    merged_df = bias_df.merge(articles_df[['event_id', 'source', 'article_text']], on=['event_id', 'source'], how=\"left\")\n",
    "    merged_df['label'] = merged_df['inf']\n",
    "\n",
    "    articles_grouped = articles_df.groupby(\"event_id\")[\"article_text\"].apply(list).to_dict()\n",
    "\n",
    "    def find_secondary_articles(row):\n",
    "        all_articles = articles_grouped.get(row[\"event_id\"], [])\n",
    "        primary_article = row[\"article_text\"]\n",
    "        secondary_articles = [article for article in all_articles if article != primary_article]\n",
    "        return secondary_articles[:2] if len(secondary_articles) >= 2 else (None, None)\n",
    "\n",
    "    ev_1_articles, ev_2_articles = zip(*merged_df.apply(find_secondary_articles, axis=1))\n",
    "    merged_df[\"ev_1\"], merged_df[\"ev_2\"] = ev_1_articles, ev_2_articles\n",
    "    merged_df = merged_df.dropna(subset=['sentence', 'ev_1', 'ev_2'])\n",
    "\n",
    "    unique_events = merged_df['event_id'].unique()\n",
    "    train_ids, rem_ids = train_test_split(unique_events, test_size=0.1, random_state=42)\n",
    "    val_ids, _ = train_test_split(rem_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_df = merged_df[merged_df['event_id'].isin(train_ids)].reset_index(drop=True)\n",
    "    val_df = merged_df[merged_df['event_id'].isin(val_ids)].reset_index(drop=True)\n",
    "\n",
    "    if local_rank == 0:\n",
    "        print(f\"Train: {len(train_df)} samples, Val: {len(val_df)} samples\")\n",
    "\n",
    "    train_dataset = BiasDataset(train_df, tokenizer, max_length=args.max_length)\n",
    "    val_dataset = BiasDataset(val_df, tokenizer, max_length=args.max_length)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=local_rank, shuffle=True)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=local_rank, shuffle=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, sampler=val_sampler, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "    model = LongformerBiLSTM(fine_tune_layers=2).to(device)\n",
    "    model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)\n",
    "\n",
    "    criterion = get_weighted_loss(train_df, device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, args.grad_accum_steps)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Perform validation after each epoch\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Only rank 0 prints results and saves checkpoints\n",
    "        if local_rank == 0:\n",
    "            print(f\"\\nEpoch {epoch}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_acc:.4f} | Precision: {val_prec:.4f} | Recall: {val_rec:.4f} | F1-score: {val_f1:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                \"model_state_dict\": model.module.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            checkpoint_path = os.path.join(args.output_dir, f\"model_ddp_epoch_{epoch}.pth\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    \n",
    "# -----------------------------\n",
    "# Main function and argument parsing\n",
    "# -----------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"DDP Training for Longformer+BiLSTM with Event Context\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./checkpoints\", help=\"Directory to save checkpoints\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size per GPU\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number of workers for DataLoader\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Total number of epochs\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-5, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-2, help=\"Weight decay\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=1024, help=\"Maximum sequence length\")\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=512, help=\"Hidden dimension for BiLSTM\")\n",
    "    parser.add_argument(\"--num_layers\", type=int, default=6, help=\"Number of BiLSTM layers\")\n",
    "    parser.add_argument(\"--grad_accum_steps\", type=int, default=4, help=\"Gradient accumulation steps\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    train_ddp(args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T14:48:51.111584Z",
     "iopub.status.busy": "2025-02-21T14:48:51.111251Z",
     "iopub.status.idle": "2025-02-21T17:02:14.832849Z",
     "shell.execute_reply": "2025-02-21T17:02:14.831950Z",
     "shell.execute_reply.started": "2025-02-21T14:48:51.111554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0221 14:48:54.671000 66 torch/distributed/run.py:793] \n",
      "W0221 14:48:54.671000 66 torch/distributed/run.py:793] *****************************************\n",
      "W0221 14:48:54.671000 66 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0221 14:48:54.671000 66 torch/distributed/run.py:793] *****************************************\n",
      "2025-02-21 14:49:03.291493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-21 14:49:03.291533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-21 14:49:03.481715: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-21 14:49:03.481704: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-21 14:49:03.536263: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-21 14:49:03.536268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "vocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 16.7MB/s]\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 10.6MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 7.84MB/s]\n",
      "config.json: 100%|█████████████████████████████| 694/694 [00:00<00:00, 3.87MB/s]\n",
      "Train: 7058 samples, Val: 458 samples\n",
      "pytorch_model.bin: 100%|██████████████████████| 597M/597M [00:02<00:00, 242MB/s]\n",
      "model.safetensors: 100%|██████████████████████| 597M/597M [00:03<00:00, 177MB/s]\n",
      "                                                                                \n",
      "Epoch 1:\n",
      "Train Loss: 1.0769 | Val Loss: 1.0429\n",
      "Val Accuracy: 0.7380 | Precision: 0.2000 | Recall: 0.1429 | F1-score: 0.1667\n",
      "Checkpoint saved at ./checkpoints/model_ddp_epoch_1.pth\n",
      "                                                                                \n",
      "Epoch 2:\n",
      "Train Loss: 1.0868 | Val Loss: 1.0225\n",
      "Val Accuracy: 0.6201 | Precision: 0.2527 | Recall: 0.5476 | F1-score: 0.3459\n",
      "Checkpoint saved at ./checkpoints/model_ddp_epoch_2.pth\n",
      "                                                                                \n",
      "Epoch 3:\n",
      "Train Loss: 1.0333 | Val Loss: 0.9503\n",
      "Val Accuracy: 0.7467 | Precision: 0.3750 | Recall: 0.5714 | F1-score: 0.4528\n",
      "Checkpoint saved at ./checkpoints/model_ddp_epoch_3.pth\n",
      "                                                                                \n",
      "Epoch 4:\n",
      "Train Loss: 0.9473 | Val Loss: 0.9324\n",
      "Val Accuracy: 0.7293 | Precision: 0.3529 | Recall: 0.5714 | F1-score: 0.4364\n",
      "Checkpoint saved at ./checkpoints/model_ddp_epoch_4.pth\n",
      "                                                                                \n",
      "Epoch 5:\n",
      "Train Loss: 0.9422 | Val Loss: 0.9380\n",
      "Val Accuracy: 0.6594 | Precision: 0.2955 | Recall: 0.6190 | F1-score: 0.4000\n",
      "Checkpoint saved at ./checkpoints/model_ddp_epoch_5.pth\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ddp_longformer_bilstm.py \\\n",
    "    --output_dir ./checkpoints \\\n",
    "    --batch_size 8 \\\n",
    "    --epochs 5 \\\n",
    "    --grad_accum_steps 4\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6498846,
     "sourceId": 10496329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
